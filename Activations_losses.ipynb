{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras, tensorflow\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the standard activations functions used in depp neural networks and we will input a list of negative and positive numbers that we will convert to numpy arrays and output the result of the activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    z = np.asarray(z)\n",
    "    f = np.exp(z)\n",
    "    sum = np.sum(np.exp(z), axis = 0)\n",
    "\n",
    "    return f/sum\n",
    "\n",
    "def sigmoid(z):\n",
    "    z = np.asarray(z)\n",
    "    f = 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "    return f    \n",
    "\n",
    "def relu(z):\n",
    "    z = np.asarray(z)\n",
    "    f = np.maximum(0.0, z)\n",
    "\n",
    "    return f\n",
    "\n",
    "def leaky_relu(z):\n",
    "  z = np.asarray(z)  \n",
    "  f = [max(0.05*value,value) for value in z]\n",
    "  return np.array(f, dtype=float)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax [0.03911257 0.78559703 0.17529039] 1.0\n",
      "Sigmoid [0.26894142 0.88079708 0.62245933] 1.772197830549732\n",
      "Relu [0.  2.  0.5] 2.5\n",
      "Relu [-0.05  2.    0.5 ] 2.45\n"
     ]
    }
   ],
   "source": [
    "z = [-1, 2, 0.5]\n",
    "softmax_z = softmax(z)\n",
    "sigmoid_z = sigmoid(z)\n",
    "relu_z = relu(z)\n",
    "leaky_relu_z = leaky_relu(z)\n",
    "\n",
    "print('Softmax',softmax_z, np.sum(softmax_z, axis = 0))\n",
    "print('Sigmoid',sigmoid_z, np.sum(sigmoid_z, axis = 0))\n",
    "print('Relu',relu_z, np.sum(relu_z, axis = 0))    \n",
    "print('Relu',leaky_relu_z, np.sum(leaky_relu_z, axis = 0))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will import these function definitions from Keras and compare the two outputs, first we have to convert our list into a numpy array and then to a tensorflow format tensor that can then be passed into it for getting back the activation function outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax tf.Tensor([0.03911257 0.78559703 0.17529039], shape=(3,), dtype=float64) 1.0\n",
      "Sigmoid tf.Tensor([0.26894142 0.88079708 0.62245933], shape=(3,), dtype=float64) 1.772197830549732\n",
      "Relu tf.Tensor([0.  2.  0.5], shape=(3,), dtype=float64) 2.5\n",
      "Relu tf.Tensor([-0.3  2.   0.5], shape=(3,), dtype=float32) 2.2\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K \n",
    "zarr = tensorflow.convert_to_tensor(np.asarray(z))\n",
    "keras_softmax_z = K.softmax(zarr)\n",
    "keras_sigmoid_z = K.sigmoid(zarr)\n",
    "keras_relu_z = K.relu(zarr)\n",
    "\n",
    "\n",
    "\n",
    "print('Softmax',keras_softmax_z, np.sum(keras_softmax_z, axis = 0))\n",
    "print('Sigmoid',keras_sigmoid_z, np.sum(keras_sigmoid_z, axis = 0))\n",
    "print('Relu',keras_relu_z, np.sum(keras_relu_z, axis = 0))    \n",
    "print('Relu',keras_leaky_relu_z, np.sum(keras_leaky_relu_z, axis = 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard loss functions are mean absolute error, mean squared error, binary cross entropy, categorical cross entropy. In the code cell below we define these errors and apply then on y_true and y_pred output vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(y_true, y_pred):\n",
    "\n",
    "      y_true = np.asarray(y_true)\n",
    "      y_pred = np.asarray(y_pred)\n",
    "\n",
    "      f =  np.mean(np.abs(y_true - y_pred), axis = 0)\n",
    "      return f\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "\n",
    "      y_true = np.asarray(y_true)\n",
    "      y_pred = np.asarray(y_pred)\n",
    "      if np.any(y_true < 0) or np.any(y_pred < 0):\n",
    "        raise ValueError('Negative no attlowed')\n",
    "      f =  np.mean((y_true - y_pred) * (y_true - y_pred), axis = 0)\n",
    "\n",
    "      return f \n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "\n",
    "      y_true = np.asarray(y_true)\n",
    "      y_pred = np.asarray(y_pred)\n",
    "      if np.any(y_true < 0) or np.any(y_pred < 0):\n",
    "        raise ValueError('Negative no attlowed')\n",
    "      f1 = (1 - y_true) * np.log( 1 - y_pred) \n",
    "      f2 = y_true * np.log(y_pred)\n",
    "    \n",
    "      f = -np.mean(f1 + f2, axis = 0)\n",
    "\n",
    "      return f\n",
    " \n",
    "def categorical_cross_entropy(y_true, y_pred):\n",
    "\n",
    "      y_true = np.asarray(y_true)\n",
    "      y_pred = np.asarray(y_pred)\n",
    "      if np.any(y_true < 0) or np.any(y_pred < 0):\n",
    "        raise ValueError('Negative no attlowed') \n",
    "      sum_true= np.sum(y_true, axis = 0)\n",
    "      sum_pred= np.sum(y_pred, axis = 0) \n",
    "      y_true = y_true/sum_true\n",
    "      y_pred = y_pred/sum_pred\n",
    "      f = -np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "      return f\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE 0.6000000000000001\n",
      "MSE 0.48666666666666675\n",
      "BE 1.3391278403619908\n",
      "CE 2.3025850929940455\n"
     ]
    }
   ],
   "source": [
    "y_true = [1,0,0]\n",
    "y_pred = [0.1,0.8,0.1]\n",
    "\n",
    "print('MAE',mean_absolute_error(y_true, y_pred) )\n",
    "print('MSE',mean_squared_error(y_true, y_pred) )\n",
    "print('BE',binary_cross_entropy(y_true, y_pred) )\n",
    "print('CE',categorical_cross_entropy(y_true, y_pred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE 0.48666666666666675\n",
      "MSE 0.48666666666666675\n",
      "BE 1.3391273033251643\n",
      "CE 2.3025850929940455\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BE = tensorflow.keras.losses.BinaryCrossentropy()\n",
    "CE = tensorflow.keras.losses.CategoricalCrossentropy()\n",
    "MSE = tensorflow.keras.losses.MeanSquaredError()\n",
    "MAE = tensorflow.keras.losses.MeanAbsoluteError()\n",
    "\n",
    "y_true_arr = tensorflow.convert_to_tensor(np.asarray(y_true))\n",
    "y_pred_arr = tensorflow.convert_to_tensor(np.asarray(y_pred))\n",
    "\n",
    "\n",
    "loss_mae = MSE(y_true_arr, y_pred_arr).numpy()\n",
    "loss_mse = MSE(y_true_arr, y_pred_arr).numpy()\n",
    "loss_be = BE(y_true_arr, y_pred_arr).numpy()\n",
    "loss_ce = CE(y_true_arr, y_pred_arr).numpy()\n",
    "\n",
    "\n",
    "print('MAE',loss_mae )\n",
    "print('MSE',loss_mse )\n",
    "print('BE',loss_be )\n",
    "print('CE',loss_ce )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('naparienv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e140276ae15c1be7b597ddfede76c3757c35dc6a4240f18994000df39384733e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
